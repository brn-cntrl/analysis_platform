{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a4bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T17:47:28.434782Z",
     "iopub.status.busy": "2025-10-09T17:47:28.434649Z",
     "iopub.status.idle": "2025-10-09T17:47:29.156262Z",
     "shell.execute_reply": "2025-10-09T17:47:29.155849Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  \n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure paths\n",
    "DATA_FOLDER = 'data'\n",
    "OUTPUT_FOLDER = 'data/outputs'\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Initialize results structure\n",
    "results = {\n",
    "    'status': 'processing',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'errors': [],\n",
    "    'warnings': [],\n",
    "    'ground_truth': {},\n",
    "    'markers': {},\n",
    "    'analysis': {},\n",
    "    'plots': []\n",
    "}\n",
    "\n",
    "from analysis_utils import (\n",
    "    prepare_event_markers_timestamps,\n",
    "    find_timestamp_offset,\n",
    "    extract_window_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d62722e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T17:47:29.525064Z",
     "iopub.status.busy": "2025-10-09T17:47:29.524939Z",
     "iopub.status.idle": "2025-10-09T17:47:29.528950Z",
     "shell.execute_reply": "2025-10-09T17:47:29.528576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Analysis complete! Results saved to data/outputs/results.json\n",
      "Status: completed\n",
      "Plots generated: 3\n"
     ]
    }
   ],
   "source": [
    "results['status'] = 'completed' if len(results['errors']) == 0 else 'completed_with_errors'\n",
    "results_path = os.path.join(OUTPUT_FOLDER, 'results.json')\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Analysis complete! Results saved to {results_path}\")\n",
    "print(f\"Status: {results['status']}\")\n",
    "print(f\"Plots generated: {len(results['plots'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1. LOADING CONFIGURATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find the most recent subject folder\n",
    "subject_folders = []\n",
    "for item in os.listdir(DATA_FOLDER):\n",
    "    item_path = os.path.join(DATA_FOLDER, item)\n",
    "    if os.path.isdir(item_path) and item not in ['outputs', 'test_temp']:\n",
    "        manifest_path = os.path.join(item_path, 'file_manifest.json')\n",
    "        if os.path.exists(manifest_path):\n",
    "            mtime = os.path.getmtime(item_path)\n",
    "            subject_folders.append((item, item_path, mtime))\n",
    "\n",
    "if not subject_folders:\n",
    "    error_msg = \"No subject folder with manifest found\"\n",
    "    print(f\"ERROR: {error_msg}\")\n",
    "    results['errors'].append(error_msg)\n",
    "    results['status'] = 'failed'\n",
    "else:\n",
    "    # Sort by modification time and get the most recent\n",
    "    subject_folders.sort(key=lambda x: x[2], reverse=True)\n",
    "    folder_name, subject_folder, _ = subject_folders[0]\n",
    "    \n",
    "    print(f\"Using subject folder: {folder_name}\")\n",
    "    \n",
    "    # Load manifest\n",
    "    manifest_path = os.path.join(subject_folder, 'file_manifest.json')\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Manifest loaded\")\n",
    "    print(f\"  EmotiBit files: {len(manifest.get('emotibit_files', []))}\")\n",
    "    print(f\"  Event markers: {'Yes' if manifest.get('event_markers') else 'No'}\")\n",
    "    \n",
    "    # Get analysis configuration\n",
    "    analysis_config = manifest.get('analysis_config', {})\n",
    "    selected_metrics = analysis_config.get('selected_metrics', [])\n",
    "    baseline_window = analysis_config.get('baseline_window', {})\n",
    "    task_window = analysis_config.get('task_window', {})\n",
    "    \n",
    "    print(f\"  Selected metrics: {selected_metrics}\")\n",
    "    print(f\"  Baseline window: {baseline_window.get('eventMarker', 'Not configured')}\")\n",
    "    print(f\"  Task window: {task_window.get('eventMarker', 'Not configured')}\")\n",
    "    \n",
    "    if not selected_metrics or not baseline_window.get('eventMarker') or not task_window.get('eventMarker'):\n",
    "        results['warnings'].append('Analysis configuration incomplete - using defaults')\n",
    "        print(\"  ⚠ Warning: Analysis configuration incomplete\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD EVENT MARKERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n2. LOADING EVENT MARKERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    if manifest.get('event_markers'):\n",
    "        event_markers_path = manifest['event_markers']['path']\n",
    "        print(f\"Loading from: {event_markers_path}\")\n",
    "        \n",
    "        df_markers = pd.read_csv(event_markers_path)\n",
    "        print(f\"✓ Loaded {df_markers.shape[0]} rows\")\n",
    "        print(f\"  Columns: {df_markers.columns.tolist()}\")\n",
    "        \n",
    "        # Prepare timestamps\n",
    "        df_markers = prepare_event_markers_timestamps(df_markers)\n",
    "        \n",
    "        # Store results\n",
    "        results['markers'] = {\n",
    "            'shape': df_markers.shape,\n",
    "            'columns': list(df_markers.columns),\n",
    "            'head': df_markers.head(10).to_dict('records')\n",
    "        }\n",
    "        \n",
    "        if 'condition' in df_markers.columns:\n",
    "            results['markers']['conditions'] = df_markers['condition'].value_counts().to_dict()\n",
    "        \n",
    "    else:\n",
    "        raise FileNotFoundError(\"No event markers file in manifest\")\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = f\"Error loading event markers: {str(e)}\"\n",
    "    print(f\"ERROR: {error_msg}\")\n",
    "    results['errors'].append(error_msg)\n",
    "    df_markers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_markers is not None and selected_metrics:\n",
    "    \n",
    "    print(\"\\n3. ANALYZING SELECTED METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in selected_metrics:\n",
    "        print(f\"\\nAnalyzing metric: {metric}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Find the metric file\n",
    "            metric_file = None\n",
    "            for emotibit_file in manifest['emotibit_files']:\n",
    "                if f'_{metric}.csv' in emotibit_file['filename']:\n",
    "                    metric_file = emotibit_file['path']\n",
    "                    break\n",
    "            \n",
    "            if not metric_file:\n",
    "                print(f\"  ⚠ Warning: File for metric {metric} not found - skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Loading: {os.path.basename(metric_file)}\")\n",
    "            df_metric = pd.read_csv(metric_file)\n",
    "            print(f\"  ✓ Loaded {df_metric.shape[0]} rows\")\n",
    "            \n",
    "            # Calculate offset\n",
    "            print(f\"  Calculating timestamp offset...\")\n",
    "            offset = find_timestamp_offset(df_markers, df_metric)\n",
    "            \n",
    "            # Extract baseline window data\n",
    "            print(f\"\\n  Extracting BASELINE window data...\")\n",
    "            baseline_data = extract_window_data(df_metric, df_markers, offset, baseline_window)\n",
    "            \n",
    "            # Extract task window data\n",
    "            print(f\"\\n  Extracting TASK window data...\")\n",
    "            task_data = extract_window_data(df_metric, df_markers, offset, task_window)\n",
    "            \n",
    "            if len(baseline_data) == 0 or len(task_data) == 0:\n",
    "                print(f\"  ⚠ Warning: Insufficient data for comparison - skipping {metric}\")\n",
    "                continue\n",
    "            \n",
    "            # Get metric column (last column)\n",
    "            metric_col = df_metric.columns[-1]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            baseline_values = baseline_data[metric_col].dropna()\n",
    "            task_values = task_data[metric_col].dropna()\n",
    "            \n",
    "            baseline_stats = {\n",
    "                'mean': float(baseline_values.mean()),\n",
    "                'std': float(baseline_values.std()),\n",
    "                'min': float(baseline_values.min()),\n",
    "                'max': float(baseline_values.max()),\n",
    "                'count': int(len(baseline_values))\n",
    "            }\n",
    "            \n",
    "            task_stats = {\n",
    "                'mean': float(task_values.mean()),\n",
    "                'std': float(task_values.std()),\n",
    "                'min': float(task_values.min()),\n",
    "                'max': float(task_values.max()),\n",
    "                'count': int(len(task_values))\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n  Baseline {metric}: mean={baseline_stats['mean']:.2f}, std={baseline_stats['std']:.2f}, n={baseline_stats['count']}\")\n",
    "            print(f\"  Task {metric}: mean={task_stats['mean']:.2f}, std={task_stats['std']:.2f}, n={task_stats['count']}\")\n",
    "            \n",
    "            # Store analysis results\n",
    "            results['analysis'][metric] = {\n",
    "                'baseline': baseline_stats,\n",
    "                'task': task_stats,\n",
    "                'difference': task_stats['mean'] - baseline_stats['mean'],\n",
    "                'percent_change': ((task_stats['mean'] - baseline_stats['mean']) / baseline_stats['mean'] * 100) if baseline_stats['mean'] != 0 else 0\n",
    "            }\n",
    "            \n",
    "            # ================================================================\n",
    "            # CREATE VISUALIZATIONS\n",
    "            # ================================================================\n",
    "            \n",
    "            print(f\"\\n  Creating visualizations...\")\n",
    "            \n",
    "            # Plot 1: Comparison bar plot\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            categories = ['Baseline', 'Task']\n",
    "            means = [baseline_stats['mean'], task_stats['mean']]\n",
    "            stds = [baseline_stats['std'], task_stats['std']]\n",
    "            \n",
    "            bars = ax.bar(categories, means, yerr=stds, capsize=10, \n",
    "                         color=['#4CAF50', '#2196F3'], alpha=0.7, edgecolor='black')\n",
    "            \n",
    "            ax.set_ylabel(f'{metric} Value', fontsize=12)\n",
    "            ax.set_title(f'{metric}: Baseline vs Task Comparison', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "                ax.text(i, mean + std + 0.05 * max(means), f'{mean:.2f}±{std:.2f}',\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot1_path = os.path.join(OUTPUT_FOLDER, f'{metric}_comparison.png')\n",
    "            plt.savefig(plot1_path, dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            results['plots'].append({\n",
    "                'name': f'{metric} Comparison',\n",
    "                'path': plot1_path,\n",
    "                'filename': f'{metric}_comparison.png'\n",
    "            })\n",
    "            print(f\"    ✓ Saved: {metric}_comparison.png\")\n",
    "            \n",
    "            # Plot 2: Time series overlay\n",
    "            fig, ax = plt.subplots(figsize=(14, 6))\n",
    "            \n",
    "            # Plot baseline data\n",
    "            baseline_times = np.arange(len(baseline_values))\n",
    "            ax.plot(baseline_times, baseline_values, linewidth=0.8, \n",
    "                   color='#4CAF50', alpha=0.7, label='Baseline')\n",
    "            \n",
    "            # Plot task data (offset on x-axis for clarity)\n",
    "            task_times = np.arange(len(task_values)) + len(baseline_values) + 100\n",
    "            ax.plot(task_times, task_values, linewidth=0.8, \n",
    "                   color='#2196F3', alpha=0.7, label='Task')\n",
    "            \n",
    "            # Add vertical separator\n",
    "            separator_x = len(baseline_values) + 50\n",
    "            ax.axvline(x=separator_x, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "            \n",
    "            ax.set_xlabel('Sample Index', fontsize=12)\n",
    "            ax.set_ylabel(f'{metric} Value', fontsize=12)\n",
    "            ax.set_title(f'{metric} Time Series: Baseline vs Task', fontsize=14, fontweight='bold')\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot2_path = os.path.join(OUTPUT_FOLDER, f'{metric}_timeseries.png')\n",
    "            plt.savefig(plot2_path, dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            results['plots'].append({\n",
    "                'name': f'{metric} Time Series',\n",
    "                'path': plot2_path,\n",
    "                'filename': f'{metric}_timeseries.png'\n",
    "            })\n",
    "            print(f\"    ✓ Saved: {metric}_timeseries.png\")\n",
    "            \n",
    "            # Plot 3: Distribution comparison (box plots)\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            data_to_plot = [baseline_values, task_values]\n",
    "            bp = ax.boxplot(data_to_plot, labels=categories, patch_artist=True,\n",
    "                           showmeans=True, meanline=True)\n",
    "            \n",
    "            # Color the boxes\n",
    "            colors = ['#4CAF50', '#2196F3']\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            \n",
    "            ax.set_ylabel(f'{metric} Value', fontsize=12)\n",
    "            ax.set_title(f'{metric} Distribution: Baseline vs Task', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plot3_path = os.path.join(OUTPUT_FOLDER, f'{metric}_distribution.png')\n",
    "            plt.savefig(plot3_path, dpi=100, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            results['plots'].append({\n",
    "                'name': f'{metric} Distribution',\n",
    "                'path': plot3_path,\n",
    "                'filename': f'{metric}_distribution.png'\n",
    "            })\n",
    "            print(f\"    ✓ Saved: {metric}_distribution.png\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error analyzing {metric}: {str(e)}\"\n",
    "            print(f\"  ERROR: {error_msg}\")\n",
    "            results['errors'].append(error_msg)\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠ Skipping analysis - no event markers or metrics selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. SAVING RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results['status'] = 'completed' if len(results['errors']) == 0 else 'completed_with_errors'\n",
    "results_path = os.path.join(OUTPUT_FOLDER, 'results.json')\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Analysis complete!\")\n",
    "print(f\"  Status: {results['status']}\")\n",
    "print(f\"  Plots generated: {len(results['plots'])}\")\n",
    "print(f\"  Metrics analyzed: {len(results.get('analysis', {}))}\")\n",
    "if results['errors']:\n",
    "    print(f\"  Errors: {len(results['errors'])}\")\n",
    "if results['warnings']:\n",
    "    print(f\"  Warnings: {len(results['warnings'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
